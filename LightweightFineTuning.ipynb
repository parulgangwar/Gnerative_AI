{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8a4d7e",
   "metadata": {},
   "source": [
    "As a part of Generative Artifcial Intelligence Nanodegree program by Udacity, this is my first project on \"Apply Lightweight Fine-Tuning to a Foundation Model\" Lightweight fine-tuning is one of the most important techniques for adapting foundation models, because it allows is to modify foundation models for \n",
    "our needs without needing substantial computational resources.\n",
    "\n",
    "Parameter-efficient fine-tuning is a technique to use pre-trained machine learning models with minimal additional data or computational resources. Basically in this technique, we use specific parameters rather than retraining the entire model. This approach, often used in transfer learning, accelerates convergence and enhances generalization, particularly in NLP, computer vision, and related fields.\n",
    "\n",
    "In this project, I've applied parameter-efficient fine-tuning using the Hugging Face peft library. In order to do, I've followed following steps.\n",
    "\n",
    "Step 1: Load a Pre-trained Model and Evaluate Its Performance\n",
    "I have used thee \"distilbert-base-uncased\" model from Hugging Face peft library. I  have used the subset of Amazon Polarity dataset. As entire dataset was quite heavy.\n",
    "In this step, I have prepared the dataset and have evaluated the pre-trained model's performance on the validation dataset.\n",
    "\n",
    "Step 2: Perform Parameter-Efficient Fine-Tuning Using the Pre-trained Model\n",
    "In this step, I've defined training arguments for fine-tuning and have also Fine-tuned the pre-trained model on the training subset.\n",
    "I've also saved the fine-tuned model.\n",
    "\n",
    "Step 3: Perform Inference Using the Fine-Tuned Model and Compare Its Performance to the Original Model\n",
    "In this step, I've loaded the fine-tuned model and have evaluated the fine-tuned model's performance on the validation dataset.\n",
    "I've also compared the trainer evaluation results with the final fine-tuned evaluation results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f5c8100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/student/.local/lib/python3.10/site-packages (1.5.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.36.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: filelock in /home/student/.local/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/student/.local/lib/python3.10/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/student/.local/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/student/.local/lib/python3.10/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/student/.local/lib/python3.10/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/student/.local/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: requests in /home/student/.local/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/student/.local/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pandas in /home/student/.local/lib/python3.10/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /home/student/.local/lib/python3.10/site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/student/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/.local/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/student/.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/student/.local/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/student/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install scikit-learn torch transformers datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c197926b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results on Trainer: {'eval_loss': 0.6990557909011841, 'eval_accuracy': 0.395, 'eval_runtime': 1.4971, 'eval_samples_per_second': 267.179, 'eval_steps_per_second': 9.351}\n"
     ]
    }
   ],
   "source": [
    "# This step is to import libaries\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# This step is to create tokenizer from pre-trained model \"distilbert-base-uncased\"\n",
    "foundation_model_is =\"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(foundation_model_is)\n",
    "foundation_model = AutoModelForSequenceClassification.from_pretrained(foundation_model, num_labels=2)\n",
    "\n",
    "# This step is to load Amazon Polarity dataset and to create a small subset for training and validation\n",
    "ds_amazon_polarity = load_dataset(\"amazon_polarity\")\n",
    "train_dataset = ds_amazon_polarity[\"train\"].shuffle(seed=42).select(range(2000))\n",
    "val_dataset = ds_amazon_polarity[\"test\"].shuffle(seed=42).select(range(400))\n",
    "\n",
    "# This step is to Tokenize datasets\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"content\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "metric = load_metric(\"accuracy\", trust_remote_code=True)\n",
    "\n",
    "def compute_metrics(eval_op):\n",
    "    predictions = eval_op.predictions.argmax(-1)\n",
    "    return metric.compute(predictions=predictions, references=eval_op.label_ids)\n",
    "\n",
    "# This step are to evaluate Foundation model which is \"distilbert-base-uncased\"\n",
    "foundation_training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_eval_batch_size=30,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "foundation_trainer = Trainer(\n",
    "    model=foundation_model,\n",
    "    args=foundation_training_args,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluation results on Foundation model\n",
    "foundation_evaluation_result = foundation_trainer.evaluate()\n",
    "print(f\"Evaluation results on Trainer: {foundation_evaluation_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These steps are to train and save PEFT model\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=\"./peft_results\",\n",
    "    num_train_epochs=1, \n",
    "    per_device_train_batch_size=30,\n",
    "    per_device_eval_batch_size=30,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./peft_logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "peft_model_trainer = Trainer(\n",
    "    model=foundation_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "peft_model_trainer.train()\n",
    "peft_model_trainer.save_model(\"./peft_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfce4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step is to load the saved PEFT model and evaluate the model\n",
    "peft_model = AutoModelForSequenceClassification.from_pretrained(\"./peft_model\")\n",
    "\n",
    "# Evaluate the PEFT model\n",
    "peft_evaluation_args = TrainingArguments(\n",
    "    output_dir=\"./peft_results\",\n",
    "    per_device_eval_batch_size=30,\n",
    "    logging_dir=\"./peft_logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "peft_evaluation_result = peft_trainer.evaluate()\n",
    "\n",
    "# This step is to show results with PEFT model\n",
    "print(f\"Evaluation results on PEFT Model: {peft_evaluation_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a42d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step is to show results of foundatin model and PEFT model\n",
    "print(f\"Please see below to get compariosn details of results between Foundation and PEFT model:\")\n",
    "print(f\"Foundation model Evaluation Results: {foundation_evaluation_result['eval_accuracy']}\")\n",
    "print(f\"PEFT model Evaluation Results: {peft_evaluation_result['eval_accuracy']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
